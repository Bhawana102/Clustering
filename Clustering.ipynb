{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQYUoxCszX8dQPyiwXMLrz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhawana102/Clustering/blob/main/Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW6Z_nCrubgX"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "!pip install pycaret &> /dev/null\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pycaret.datasets import get_data\n",
        "from pycaret.clustering import *\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "# Load a sample dataset\n",
        "data = get_data('wholesale')\n",
        "# data = data.drop(columns=['species'])\n",
        "\n",
        "# Define the preprocessing methods\n",
        "preprocessing_methods = ['None', 'Normalize', 'PCA', 'Transform', 'PCA+Transform', 'PCA+Transform+Normalize']\n",
        "\n",
        "# Define the clustering techniques\n",
        "clustering_techniques = ['kmeans', 'hclust', 'meanshift']  # Updated clustering techniques\n",
        "\n",
        "# Define the number of clusters\n",
        "num_clusters = [3, 4, 5]\n",
        "\n",
        "# Initialize dictionaries to store the results matrices for each clustering technique\n",
        "results_matrices = {}"
      ],
      "metadata": {
        "id": "A7QC1HM6ugKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cluster_method in clustering_techniques:\n",
        "    # Initialize an empty DataFrame to store results for this clustering method\n",
        "    results_matrix = np.zeros((3, len(preprocessing_methods)), dtype=float)\n",
        "\n",
        "    # Iterate over each preprocessing method\n",
        "    for i, prep_method in enumerate(preprocessing_methods):\n",
        "        # Apply the preprocessing method\n",
        "        if 'Normalize' in prep_method:\n",
        "            exp_clf_setup = setup(data, normalize=True, verbose=False)\n",
        "        elif 'PCA' in prep_method:\n",
        "            exp_clf_setup = setup(data, pca=True, pca_components=2, verbose=False)\n",
        "        elif 'Transform' in prep_method:\n",
        "            exp_clf_setup = setup(data, transformation=True, transformation_method='yeo-johnson', verbose=False)\n",
        "        else:\n",
        "            exp_clf_setup = setup(data, verbose=False)\n",
        "\n",
        "        preprocessed_data = get_config('X')\n",
        "\n",
        "        # Initialize lists to store the scores for each number of clusters\n",
        "        silhouette_scores = []\n",
        "        calinski_scores = []\n",
        "        davies_scores = []\n",
        "\n",
        "        # Iterate over each number of clusters\n",
        "        for n_clusters in num_clusters:\n",
        "            # Initialize the clustering model\n",
        "            if cluster_method == 'kmeans':\n",
        "                model = create_model('kmeans', num_clusters=n_clusters)\n",
        "            elif cluster_method == 'hclust':\n",
        "                model = create_model('hclust', linkage='ward', num_clusters=n_clusters)\n",
        "            elif cluster_method == 'meanshift':\n",
        "                model = create_model('meanshift')\n",
        "\n",
        "            # Fit the model\n",
        "            model.fit(preprocessed_data)\n",
        "            save_model(model, f'{cluster_method}_{prep_method}_model')\n",
        "\n",
        "            # Get cluster labels\n",
        "            labels = model.labels_\n",
        "             # Calculate silhouette score\n",
        "            silhouette = silhouette_score(preprocessed_data, labels)\n",
        "            silhouette_scores.append(silhouette)\n",
        "\n",
        "            # Calculate calinski harabasz score\n",
        "            calinski = calinski_harabasz_score(preprocessed_data, labels)\n",
        "            calinski_scores.append(calinski)\n",
        "\n",
        "            # Calculate davies bouldin score\n",
        "            davies = davies_bouldin_score(preprocessed_data, labels)\n",
        "            davies_scores.append(davies)\n",
        "\n",
        "        # Store the scores in the results matrix\n",
        "        results_matrix[0, i] = np.mean(silhouette_scores)\n",
        "        results_matrix[1, i] = np.mean(calinski_scores)\n",
        "        results_matrix[2, i] = np.mean(davies_scores)\n",
        "\n",
        "    # Store the results matrix in the dictionary\n",
        "    results_matrices[cluster_method] = results_matrix\n",
        "\n",
        "# Writing each matrix to a CSV file\n",
        "for algorithm, matrix in results_matrices.items():\n",
        "    np.savetxt(f'{algorithm}_results.csv', matrix, delimiter=',', header=','.join(preprocessing_methods), comments='')"
      ],
      "metadata": {
        "id": "Pj3RFJUzugNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6zWiKB-HugPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZW_VpxpPugR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CSRqHXyuugUw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}